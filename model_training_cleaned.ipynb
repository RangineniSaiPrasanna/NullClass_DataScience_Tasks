{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c490fcb5-6f02-4c4f-b821-5bf49a218d3d",
   "metadata": {
    "id": "c490fcb5-6f02-4c4f-b821-5bf49a218d3d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b55922ae-3e1b-4ab3-9ffd-77c66db3a358",
   "metadata": {
    "id": "b55922ae-3e1b-4ab3-9ffd-77c66db3a358"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "#import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/rgukt/TASK1/IMDB Dataset.csv',on_bad_lines='skip',quoting=3)\n",
    "df = df.sample(n=1000, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22159d6f-b368-40ca-80d0-68e5ad7ce1f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3dbde8dedb154e9e86fefa6352ec1ada",
      "5ae25d7791254de7a879f67c2fbf9945",
      "6d29aef41de84173bc5656109606d4d3",
      "0f7d12f4a4a8400b9eb927abafc7c392",
      "a25b8b9472b74d1d8f98045f190e0ef9",
      "2d76004a970342eaa51a9be8c915ecbf",
      "20bc3b94ad8442218842ca4a852b2c44",
      "14dc226676c342ffb40b68026fb219a9",
      "f0f718b5a8a24645a421e569a96db7ba",
      "8680ba6a020648e18d7a7c697266f925",
      "b7f4fdece9c84af7adbbef08b274813a",
      "78807915fb8b4b85b78d23141bd03393",
      "f7de103bb7dc43e98001821bd39327aa",
      "a66a557d458f47b0a3e8877e42f34694",
      "52a9c67a165244efbcdca86899d659d6",
      "424e157a49bc45ea8cbe1ebef1badca7",
      "4088904a57ca450fa639f7b62d91460d",
      "2795c47041c84db0bcd1e7cb71877e0d",
      "ddcbd01bb009455d8faf463541080d31",
      "5028787a665742a68fe00430632b2d29",
      "862f7141e82b4e3990e0e3920fa0465c",
      "593dd7435b4b481db51035b78991b865",
      "8e28fccb5209420a9c5b44f30cbd2b3e",
      "3e2b2abfb6e34fffb1321b955909b790",
      "36a0afee87534fee81656b6424688edf",
      "35da657ba79c4b29962b8610a35f3c42",
      "65a2a8b676714da7bc0608971cbc67bc",
      "dd3d792affc940ac8f7bbce2c7ec3bba",
      "4fb71166e9a148aab0fe35eac430f025",
      "fa2ee106238b43c39c352d9442a2d7a6",
      "6f2340e8813a4a2bad4b753c8bba46bf",
      "ee9f449b51b34a589bf15ca192892851",
      "b43552d14ff94eda8c9683da7d268a49",
      "2d877a790efe4fb9aa801ed1f1b3b2cb",
      "a8f2513f43314c7993f361926606a800",
      "fbd0735b7f6e48b9ab06b42c05fbe186",
      "10a4ecde16f64270b03a4572692d7b75",
      "e8a475d301054a06b9d85ec8d512a99d",
      "bf7227561cf441eabdf0d3f4eefb3019",
      "8708dc858252406b99f32c0063f054bc",
      "b306e4661b4744338d54f88690b09a0b",
      "8a824b5014ce4c609c3ae4822eee8547",
      "8275751703144f06a834b6a20f86fe77",
      "73f94056651e472496d81056172e79f8",
      "f96bc5334cd649cd86988435d37b915e",
      "f1e3dd478b244d2b8decdf353ae39c94",
      "e12740d75a5d478cb02ab3d977b47b2e",
      "b826980eec564d6cb03273c2fe0e58fa",
      "fe9deff2dce644a48b7070f6680cd7da",
      "346ed5f4ded24f0a98ae9704b6478e02",
      "64f494fb42c6444bae57079c05456e8c",
      "322a008f74ed4441959e9f4bffb65cd4",
      "544cfa269eee481184d99a4bb6eee2bd",
      "0516664adf1c4ec8bbac6b005cd33713",
      "1179cd7b10684d1bbf1630258ad95920"
     ]
    },
    "id": "22159d6f-b368-40ca-80d0-68e5ad7ce1f5",
    "outputId": "850b522d-0e99-463d-c6f2-ed51f630b22f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_20850/3304745196.py:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/home/rgukt/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/tmp/ipykernel_20850/3304745196.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long) # Change to torch.long\n"
     ]
    }
   ],
   "source": [
    "\n",
    " # Ensure the correct file path is provided\n",
    "\n",
    "# Encode the sentiment labels\n",
    "# Replace NAType with 0 in the 'sentiment' column\n",
    "df['sentiment'] = pd.to_numeric(df['sentiment'].map({'positive': 1, 'negative': 0}), errors='coerce').astype('Int64').fillna(0)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Clean the 'review' column before tokenization\n",
    "# Remove or replace empty strings and non-string values with a placeholder.\n",
    "df['review'] = df['review'].astype(str).replace('', 'empty_string')\n",
    "\n",
    "# Tokenize the reviews\n",
    "tokens = tokenizer(df['review'].tolist(), padding=True, truncation=True, return_tensors='pt', max_length=128)\n",
    "\n",
    "# Train-test split (split tokens and labels)\n",
    "X_train, X_val, y_train, y_val = train_test_split(tokens['input_ids'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the attention masks similarly to input_ids\n",
    "train_attention_mask, val_attention_mask = train_test_split(tokens['attention_mask'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a custom Dataset class\n",
    "# ... (rest of the code remains the same)\n",
    "\n",
    "# Create a custom Dataset class\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        # Convert labels to float tensors\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long) # Change to torch.long\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = IMDBDataset({'input_ids': X_train, 'attention_mask': train_attention_mask}, y_train)\n",
    "val_dataset = IMDBDataset({'input_ids': X_val, 'attention_mask': val_attention_mask}, y_val)\n",
    "\n",
    "# ... (rest of the code remains the same)\n",
    "\n",
    "# Load the BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  # Output directory\n",
    "    num_train_epochs=1,  # Number of training epochs\n",
    "    per_device_train_batch_size=8,  # Batch size for training\n",
    "    per_device_eval_batch_size=8,  # Batch size for evaluation\n",
    "    warmup_steps=500,  # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # Strength of weight decay\n",
    "    logging_dir='./logs',  # Directory for storing logs\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,  # Model to train\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=train_dataset,  # Training dataset\n",
    "    eval_dataset=val_dataset,  # Validation dataset\n",
    "    tokenizer=tokenizer,  # Tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./imdb_model')\n",
    "tokenizer.save_pretrained('./imdb_model')\n",
    "\n",
    "# Evaluate the model\n",
    "predictions = trainer.predict(val_dataset)\n",
    "pred_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(confusion_matrix(y_val, pred_labels))\n",
    "print(classification_report(y_val, pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80edadd-7c94-42a2-8a4e-eb502670c635",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f80edadd-7c94-42a2-8a4e-eb502670c635",
    "outputId": "5873482e-b5ca-453b-d4c1-590d68bdd207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.995\n",
      "recall_score: 0.0\n",
      "f1_score: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "#print(confusion_matrix(y_val, pred_labels))\n",
    "#print(classification_report(y_val, pred_labels))\n",
    "print(\"accuracy score:\",accuracy_score(y_val, pred_labels))\n",
    "print(\"recall_score:\",recall_score(y_val, pred_labels))\n",
    "print(\"f1_score:\",f1_score(y_val, pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pr0fT1J0sQKz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pr0fT1J0sQKz",
    "outputId": "9b624d6f-f9eb-410f-df21-f4b31ca5c8a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "0    997\n",
      "1      3\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "print(df['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VY7K3NyxtTOa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VY7K3NyxtTOa",
    "outputId": "13dc9c21-f5e1-4871-e855-24184025f514"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "<ipython-input-7-6e3fe226155c>:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute weights\n",
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=np.unique(y_train),\n",
    "                                     y=y_train)\n",
    "\n",
    "# Convert to tensor\n",
    "weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Pass these weights to Trainer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "   output_dir='./results',  # Output directory\n",
    "    num_train_epochs=1,  # Number of training epochs\n",
    "    per_device_train_batch_size=8,  # Batch size for training\n",
    "    per_device_eval_batch_size=8,  # Batch size for evaluation\n",
    "    warmup_steps=500,  # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,  # Strength of weight decay\n",
    "    logging_dir='./logs',  # Directory for storing logs\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    ")\n",
    "# Wrap model with weights\n",
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Redefine trainer with weights\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    #compute_metrics=your_metrics_function,\n",
    "    # Set loss weights\n",
    "    data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]),\n",
    "                                'attention_mask': torch.stack([f['attention_mask'] for f in data]),\n",
    "                                'labels': torch.tensor([f['labels'] for f in data])}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z_Sw-90NPPmF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "z_Sw-90NPPmF",
    "outputId": "d7968bc4-bfbb-4470-ca69-72d6f87b83fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-5228a0491b48>:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long) # Change to torch.long\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 08:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.362400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.307400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.135200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.1614693620055914, metrics={'train_runtime': 542.8895, 'train_samples_per_second': 1.474, 'train_steps_per_second': 0.184, 'total_flos': 21788884272000.0, 'train_loss': 0.1614693620055914, 'epoch': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retraining the model\n",
    "# Train the model again on the balanced dataset\n",
    "train_dataset = IMDBDataset({'input_ids': X_train, 'attention_mask': train_attention_mask}, y_train)\n",
    "val_dataset = IMDBDataset({'input_ids': X_val, 'attention_mask': val_attention_mask}, y_val)\n",
    "\n",
    "trainer.train()  # Re-train the model with the balanced dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w8IlGhbpT-Ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w8IlGhbpT-Ac",
    "outputId": "c2080208-1a35-4151-8c88-78feb708c825"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[199   0]\n",
      " [  1   0]]\n",
      "accuracy score: 0.995\n",
      "recall_score: 0.0\n",
      "f1_score: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_val, pred_labels))\n",
    "#print(classification_report(y_val, pred_labels))\n",
    "print(\"accuracy score:\",accuracy_score(y_val, pred_labels))\n",
    "print(\"recall_score:\",recall_score(y_val, pred_labels))\n",
    "print(\"f1_score:\",f1_score(y_val, pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6GUnJCOOVWhQ",
   "metadata": {
    "id": "6GUnJCOOVWhQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6sHREaBbVXGe",
   "metadata": {
    "id": "6sHREaBbVXGe"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('/content/IMDB Dataset.csv', encoding='utf-8', on_bad_lines='skip',quoting=3)\n",
    "\n",
    "# Drop empty/null reviews\n",
    "df = df[df['review'].notnull()]\n",
    "df = df[df['review'].str.strip() != '']\n",
    "\n",
    "# Encode labels\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r6VYptLoVXb0",
   "metadata": {
    "id": "r6VYptLoVXb0"
   },
   "outputs": [],
   "source": [
    "# Balance the dataset\n",
    "positive = df[df['sentiment'] == 1]\n",
    "negative = df[df['sentiment'] == 0].sample(len(positive), random_state=42)\n",
    "balanced_df = pd.concat([positive, negative]).sample(frac=1, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qjl79Q6yVbQe",
   "metadata": {
    "id": "Qjl79Q6yVbQe"
   },
   "outputs": [],
   "source": [
    "# Step 4: Tokenize Reviews\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokens = tokenizer(\n",
    "    balanced_df['review'].tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "labels = torch.tensor(balanced_df['sentiment'].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3uIsITtFVbkU",
   "metadata": {
    "id": "3uIsITtFVbkU"
   },
   "outputs": [],
   "source": [
    "#ðŸ“¦ Step 5: Create Dataset Objects\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Create a custom Dataset class\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        # Convert labels to long tensors for CrossEntropyLoss\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Split\n",
    "train_idx, val_idx = train_test_split(range(len(labels)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = IMDBDataset({k: v[train_idx] for k, v in tokens.items()}, labels[train_idx])\n",
    "val_dataset   = IMDBDataset({k: v[val_idx] for k, v in tokens.items()}, labels[val_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quVOmvfhWw7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "quVOmvfhWw7d",
    "outputId": "8124236a-11f7-4cf6-997e-af20abb53e0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "<ipython-input-24-1f31639985e5>:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "<ipython-input-23-34ad37dcc011>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 02:56, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.754861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.703300</td>\n",
       "      <td>0.732827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-34ad37dcc011>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12, training_loss=0.6780556440353394, metrics={'train_runtime': 190.9404, 'train_samples_per_second': 0.503, 'train_steps_per_second': 0.063, 'total_flos': 5722665454080.0, 'train_loss': 0.6780556440353394, 'epoch': 2.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load Model and Define Training Args\n",
    "# Explicitly set problem_type for binary classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, problem_type=\"single_label_classification\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    ")\n",
    "\n",
    "#train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FcCqCCkiWyHa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "FcCqCCkiWyHa",
    "outputId": "a08a00b1-18ca-4cb5-d5a3-dc2e71dbf314"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-34ad37dcc011>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 6]\n",
      " [0 5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     1.0000    0.1429    0.2500         7\n",
      "         1.0     0.4545    1.0000    0.6250         5\n",
      "\n",
      "    accuracy                         0.5000        12\n",
      "   macro avg     0.7273    0.5714    0.4375        12\n",
      "weighted avg     0.7727    0.5000    0.4062        12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(val_dataset)\n",
    "preds = torch.argmax(torch.tensor(predictions.predictions), axis=1)\n",
    "\n",
    "print(confusion_matrix(labels[val_idx], preds))\n",
    "print(classification_report(labels[val_idx], preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JY35JxIDWy1Q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JY35JxIDWy1Q",
    "outputId": "3aff26d7-f100-4d50-d6b5-6584ccc3de70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 6]\n",
      " [0 5]]\n",
      "accuracy score: 0.5\n",
      "recall score: 1.0\n",
      "f1_score: 0.625\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(labels[val_idx], preds))\n",
    "print(\"accuracy score:\",accuracy_score(labels[val_idx], preds))\n",
    "print(\"recall score:\",recall_score(labels[val_idx], preds))\n",
    "print(\"f1_score:\",f1_score(labels[val_idx], preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1rK5lhwCbJuI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rK5lhwCbJuI",
    "outputId": "43177551-7eef-4cc2-ebd4-ceedeb6f12a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "0.0    34\n",
      "1.0    30\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vCW-uBTldlJN",
   "metadata": {
    "id": "vCW-uBTldlJN"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets scikit-learn -q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yAxT4ufjdlla",
   "metadata": {
    "id": "yAxT4ufjdlla"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "_BN-D07rfXHQ",
   "metadata": {
    "id": "_BN-D07rfXHQ"
   },
   "source": [
    "# # Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IfA1cD4Rdl6w",
   "metadata": {
    "id": "IfA1cD4Rdl6w"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KLCuGLKZfjTx",
   "metadata": {
    "id": "KLCuGLKZfjTx"
   },
   "source": [
    "Data Loading and Initial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gBs2aW7ddmXQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gBs2aW7ddmXQ",
    "outputId": "3392f95e-857a-4041-9aac-6fa3c4a61842"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: One class has fewer instances than the desired sample size per class. Using all available instances from the smaller class.\n",
      "Sentiment distribution in sampled and cleaned dataset (Balanced):\n",
      "sentiment\n",
      "1    30\n",
      "0    30\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Data Loading and Initial Preprocessing\n",
    "# %%\n",
    "# Load the dataset from CSV\n",
    "# Use on_bad_lines='skip' to handle potential parsing errors and quoting=3 for QUOTE_NONE\n",
    "df = pd.read_csv('/content/IMDB Dataset.csv', on_bad_lines='skip', quoting=3)\n",
    "\n",
    "# --- Fix 1: Explicitly balance the dataset after loading and cleaning ---\n",
    "# Drop empty/null reviews and empty strings first from the full dataset\n",
    "df = df[df['review'].notnull()]\n",
    "df = df[df['review'].astype(str).str.strip() != '']\n",
    "\n",
    "# Encode the sentiment labels for the cleaned full dataframe\n",
    "df['sentiment'] = pd.to_numeric(df['sentiment'].map({'positive': 1, 'negative': 0}), errors='coerce').astype('Int64').fillna(0)\n",
    "\n",
    "# Separate positive and negative reviews\n",
    "positive_df = df[df['sentiment'] == 1]\n",
    "negative_df = df[df['sentiment'] == 0]\n",
    "\n",
    "# Sample from the larger class to match the size of the smaller class, or sample a fixed number from each\n",
    "# Let's aim for a balanced sample, e.g., 500 positive and 500 negative, totaling 1000\n",
    "sample_size_per_class = 500 # Define sample size per class\n",
    "if len(positive_df) >= sample_size_per_class and len(negative_df) >= sample_size_per_class:\n",
    "    sampled_positive = positive_df.sample(n=sample_size_per_class, random_state=42)\n",
    "    sampled_negative = negative_df.sample(n=sample_size_per_class, random_state=42)\n",
    "    sampled_df = pd.concat([sampled_positive, sampled_negative]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "else:\n",
    "    # Handle case where one class has fewer than sample_size_per_class instances\n",
    "    print(\"Warning: One class has fewer instances than the desired sample size per class. Using all available instances from the smaller class.\")\n",
    "    min_class_size = min(len(positive_df), len(negative_df))\n",
    "    sampled_positive = positive_df.sample(n=min_class_size, random_state=42)\n",
    "    sampled_negative = negative_df.sample(n=min_class_size, random_state=42)\n",
    "    sampled_df = pd.concat([sampled_positive, sampled_negative]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Print value counts for the sampled and cleaned dataframe\n",
    "print(\"Sentiment distribution in sampled and cleaned dataset (Balanced):\")\n",
    "print(sampled_df['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RcgwDzJ5f43a",
   "metadata": {
    "id": "RcgwDzJ5f43a"
   },
   "source": [
    "Data Sampling (5000 samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RZsIzLPUdmvT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZsIzLPUdmvT",
    "outputId": "6ed45072-1e6a-45d4-9c18-3db2e0069e30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_ids tensor: torch.Size([60, 88])\n",
      "Shape of attention_mask tensor: torch.Size([60, 88])\n",
      "Shape of labels tensor: torch.Size([60])\n",
      "Training dataset size: 48\n",
      "Validation dataset size: 12\n",
      "\n",
      "Sentiment distribution in training dataset:\n",
      "1    24\n",
      "0    24\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sentiment distribution in validation dataset:\n",
      "0    6\n",
      "1    6\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-56-19d48138ed5b>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Tokenization and Dataset Creation\n",
    "# %%\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the reviews from the sampled DataFrame\n",
    "tokens = tokenizer(\n",
    "    sampled_df['review'].tolist(), # Use review column from sampled_df\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128, # Keeping max_length consistent for now\n",
    "    return_tensors='pt' # Return PyTorch tensors\n",
    ")\n",
    "# %%\n",
    "# Get labels from the sampled_df as a PyTorch tensor\n",
    "# The sentiment column is now guaranteed to have integer values (0 or 1) after balancing\n",
    "labels = torch.tensor(sampled_df['sentiment'].values.astype('int64'))\n",
    "\n",
    "# Print shapes to confirm data is ready\n",
    "print(\"Shape of input_ids tensor:\", tokens['input_ids'].shape)\n",
    "print(\"Shape of attention_mask tensor:\", tokens['attention_mask'].shape)\n",
    "print(\"Shape of labels tensor:\", labels.shape)\n",
    "# %%\n",
    "\n",
    "# Create a custom Dataset class\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get a single item (input_ids, attention_mask)\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        # Get the corresponding label and convert to torch.long\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        # The number of items in the dataset is the number of labels\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Train-test split (split tokens and labels)\n",
    "# Split based on indices to ensure corresponding inputs and masks are split together\n",
    "import numpy as np # Import numpy if not already imported\n",
    "\n",
    "# Get indices for splitting\n",
    "train_idx, val_idx = train_test_split(np.arange(len(labels)), test_size=0.2, random_state=42, stratify=labels) # --- Fix 2: Add stratify=labels for stratified split ---\n",
    "\n",
    "# Use indices to create training and validation datasets\n",
    "train_dataset = IMDBDataset(\n",
    "    {key: tokens[key][train_idx] for key in tokens.keys()}, # Select token data using train_idx\n",
    "    labels[train_idx] # Select labels using train_idx\n",
    ")\n",
    "val_dataset = IMDBDataset(\n",
    "    {key: tokens[key][val_idx] for key in tokens.keys()}, # Select token data using val_idx\n",
    "    labels[val_idx] # Select labels using val_idx\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Print dataset sizes to verify the split\n",
    "print(\"Training dataset size:\", len(train_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "\n",
    "# Print sentiment distribution in train and validation sets after stratified split\n",
    "print(\"\\nSentiment distribution in training dataset:\")\n",
    "train_labels_list = [train_dataset[i]['labels'].item() for i in range(len(train_dataset))]\n",
    "print(pd.Series(train_labels_list).value_counts())\n",
    "\n",
    "print(\"\\nSentiment distribution in validation dataset:\")\n",
    "val_labels_list = [val_dataset[i]['labels'].item() for i in range(len(val_dataset))]\n",
    "print(pd.Series(val_labels_list).value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1uRaDh-JjO7p",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1uRaDh-JjO7p",
    "outputId": "23fe2803-fa96-401c-814d-63a8b44fd7aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT model for sequence classification\n",
    "# Explicitly set problem_type for binary classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, problem_type=\"single_label_classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45VVMx0MjRse",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45VVMx0MjRse",
    "outputId": "e83b8703-5054-455a-b9ff-247392249342"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computed Class Weights: tensor([1., 1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Fix 3: Compute class weights and pass to the model or Trainer ---\n",
    "# We will pass them to the model's forward pass using a custom Trainer or by modifying the model's loss calculation.\n",
    "# Let's modify the model's forward pass for simplicity within the Trainer structure.\n",
    "# This often involves passing the weights to the loss function (CrossEntropyLoss) inside the model's forward method.\n",
    "# However, the standard Hugging Face Trainer doesn't have a direct argument for loss weights in TrainingArguments.\n",
    "# A common workaround is to use a custom Trainer or adjust the dataset sampling/weighting.\n",
    "# Given we've balanced the dataset, class weights might be less critical but can still help.\n",
    "# Let's compute them anyway if needed later or for comparison.\n",
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=np.unique(train_labels_list),\n",
    "                                     y=train_labels_list)\n",
    "weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(\"\\nComputed Class Weights:\", weights)\n",
    "\n",
    "# If running on GPU, move weights to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "weights = weights.to(device)\n",
    "model.to(device) # Move model to device\n",
    "\n",
    "# Note: Passing class weights directly to the Trainer is not standard.\n",
    "# It's typically done by:\n",
    "# 1. Modifying the model's forward method to use the weights in the loss calculation.\n",
    "# 2. Using a custom `compute_loss` method in the Trainer.\n",
    "# 3. Data resampling (which we partly did by balancing).\n",
    "# For simplicity and using the standard Trainer, we won't directly pass `weights` here.\n",
    "# The balancing itself should help significantly. If still needed, a custom Trainer is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8SvUVhuFjPqA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8SvUVhuFjPqA",
    "outputId": "113eaa67-dce4-4e24-d49c-bdca42146c5f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "<ipython-input-59-8d02de1b8f0c>:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',  # Directory to save results and checkpoints\n",
    "    num_train_epochs=5,  # --- Fix 4: Increased number of training epochs ---\n",
    "    per_device_train_batch_size=8,  # Batch size per GPU/CPU for training\n",
    "    per_device_eval_batch_size=8,  # Batch size per GPU/CPU for evaluation\n",
    "    warmup_steps=500,  # Number of steps for learning rate warmup\n",
    "    weight_decay=0.01,  # Strength of weight decay\n",
    "    logging_dir='./logs',  # Directory for storing logs\n",
    "    logging_steps=10,  # Log training metrics every 10 steps\n",
    "    eval_strategy='epoch', # Evaluate the model at the end of each epoch\n",
    "    save_strategy='epoch', # Save a checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True, # Load the best model (based on eval_loss) at the end of training\n",
    "    metric_for_best_model='eval_loss', # Metric to monitor for determining the best model\n",
    "    greater_is_better=False # For 'eval_loss', smaller is better\n",
    "    # --- Optional: Adjust learning rate if needed ---\n",
    "    # learning_rate=5e-5, # Default is 5e-5, can try slightly lower or higher\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Initialize the Trainer\n",
    "# Note: We are NOT directly passing 'weights' here. The model loss calculation\n",
    "# within the BertForSequenceClassification handles the loss. Balancing the dataset\n",
    "# is the primary way we address imbalance with the standard Trainer.\n",
    "trainer = Trainer(\n",
    "    model=model,  # The BERT model to train\n",
    "    args=training_args,  # The training arguments defined above\n",
    "    train_dataset=train_dataset,  # The dataset for training\n",
    "    eval_dataset=val_dataset,  # The dataset for validation (evaluation during training)\n",
    "    tokenizer=tokenizer,  # The tokenizer (used by the Trainer for potential tokenization if needed, though we pre-tokenized)\n",
    "    # You could add 'compute_metrics' here to get custom metrics during evaluation *during* training\n",
    "    # compute_metrics=compute_metrics, # Uncomment if you define a compute_metrics function\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AWkc9uvDjPyf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "AWkc9uvDjPyf",
    "outputId": "bf2aa8bb-f9ac-4ac0-e9f4-d2e782fc7ada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-56-19d48138ed5b>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 05:34, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.686752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.686800</td>\n",
       "      <td>0.677475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.686800</td>\n",
       "      <td>0.663971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.678500</td>\n",
       "      <td>0.651771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.652400</td>\n",
       "      <td>0.639128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-56-19d48138ed5b>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "<ipython-input-56-19d48138ed5b>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "<ipython-input-56-19d48138ed5b>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
      "<ipython-input-56-19d48138ed5b>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eABAXPrVjP6O",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "eABAXPrVjP6O",
    "outputId": "0b6385e5-93b1-4fc1-ee61-3c54e86c5f18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model and tokenizer...\n",
      "Model and tokenizer saved to ./imdb_model\n",
      "\n",
      "Evaluating the model on the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-56-19d48138ed5b>:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the trained model and tokenizer\n",
    "print(\"Saving model and tokenizer...\")\n",
    "model.save_pretrained('./imdb_model')\n",
    "tokenizer.save_pretrained('./imdb_model')\n",
    "print(\"Model and tokenizer saved to ./imdb_model\")\n",
    "\n",
    "# Evaluate the model on the validation dataset\n",
    "print(\"\\nEvaluating the model on the validation set...\")\n",
    "predictions = trainer.predict(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S7UjQe0UjP-Y",
   "metadata": {
    "id": "S7UjQe0UjP-Y"
   },
   "outputs": [],
   "source": [
    "# Get the predicted class IDs (index of the highest logit)\n",
    "pred_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "# Get the true labels from the validation dataset\n",
    "# We iterate through the val_dataset to get the original labels\n",
    "# val_labels_list was already created after stratified split\n",
    "true_labels = val_labels_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A6EtJ0J-jQCQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A6EtJ0J-jQCQ",
    "outputId": "e45856d4-32a8-4f15-a3d3-4fd1bd5fea20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "[[4 2]\n",
      " [0 6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.6667    0.8000         6\n",
      "           1     0.7500    1.0000    0.8571         6\n",
      "\n",
      "    accuracy                         0.8333        12\n",
      "   macro avg     0.8750    0.8333    0.8286        12\n",
      "weighted avg     0.8750    0.8333    0.8286        12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print evaluation metrics\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(confusion_matrix(true_labels, pred_labels))\n",
    "print(classification_report(true_labels, pred_labels, digits=4)) # Use digits for more precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f742r1r0jQFn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f742r1r0jQFn",
    "outputId": "ca81fe5a-f3b3-4cd0-858f-93f99140e55e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.8333333333333334\n",
      "recall score: 1.0\n",
      "f1_score: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "# Print individual metrics\n",
    "print(\"accuracy score:\", accuracy_score(true_labels, pred_labels))\n",
    "print(\"recall score:\", recall_score(true_labels, pred_labels))\n",
    "print(\"f1_score:\", f1_score(true_labels, pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m02fe9KijQIb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m02fe9KijQIb",
    "outputId": "3c83bd7d-ba54-4ead-e291-c6138a176397"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment distribution in sampled and cleaned dataset:\n",
      "sentiment\n",
      "1    30\n",
      "0    30\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# Check the sentiment distribution of the original sampled and cleaned data again (optional)\n",
    "print(\"\\nSentiment distribution in sampled and cleaned dataset:\")\n",
    "print(sampled_df['sentiment'].value_counts())\n",
    "\n",
    "# Example of using the trained model for prediction (optional)\n",
    "# ... (pre"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
